\chapter{Classification of Spectra}
\label{ch:spectra_classification}

Let’s open the collagen data set again and see how well can logistic regression predict its four classes. Straightforward, right?

\begin{figure}[h]
    \includegraphics[width=0.95\textwidth]{sp_classification-fig1.png}
    \label{fig:spectra_classification-fig1}
    \caption{In \widget{Preprocess Spectra} we also did some spectral processing: we decided to only keep columns for wavenumbers between 1500\wn and 1800\wn.}
\end{figure}

\noindent Connect \widget{Datasets}, \widget{Logistic Regression}, \widget{Predictions}, \widget{Confusion Matrix} and that's it.

\begin{figure*}
  \newcommand{\spectra}{\includegraphics[scale=0.35]{sp_classification-fig2b.png}}
  \newcommand{\confusion}{\includegraphics[scale=0.45]{sp_classification-fig2a_.png}}
  \hspace{3.5cm} \stackinset{l}{-4cm}{t}{-1cm}
  {\confusion}
  {\spectra}
  \caption{In the \widget{Confusion Matrix} we selected wrong predictions for the actual class DNA. The connected \widget{Spectra} widget displays them.}
  \vspace{-0.5cm}
  \label{fig:spectra_classification-fig2}
\end{figure*}

\noindent Let’s not forget that it is pointless to predict for the same data as we used for learning. We could either  use a \widget{Data Sampler} and connect its Sample output to \widget{Preprocess Spectra} and Remaining output to \widget{Predictions}, or obtain predictions from the \widget{Test and Score} widget.
\widget{Confusion Matrix} now shows the mistakes of the model (scored with cross-validation). We can select them and inspect them further in a \widget{Spectra} widget. Here we colored them by the predicted class (see the Menu). 

\begin{wrapfigure}{o}{1.1\textwidth}
  \centering
  \includegraphics[width=1.1\textwidth]{sp_classification-fig3.png}%
  \label{fig:spectra_classification-fig3}
\end{wrapfigure}

But how does the model make its decisions? We already inspected a different model, classification tree, where each node represents a decision on a value of a column.  \widget{Logistic regression} works differently. On the training data it computes weights for all columns (wavelengths), which are then used for prediction, where values are multiplied with weights. To see the weights, connect \widget{Logistic Regression} to a \widget{Data Table}. 

\begin{wrapfigure}{o}{\textwidth}
%  \centering
  \vspace{-0.7cm}
  \includegraphics[width=0.9\textwidth]{sp_classification-fig4.png}
  \label{fig:spectra_classification-fig4}
\end{wrapfigure}
We get a table that is hard to understand. What if we visualize it? First, \widget{Transpose} the data. Then, use \widget{Select Columns} to make the visualization prettier: in the widget remove the intercept.

Now, open \widget{Logistic Regression} and try changing its parameters. Observe the effect on the weights.

\begin{figure}[h]
\hspace{-1cm}\stackinset{r}{0\linewidth}{t}{-0.12\linewidth}
  {\includegraphics[scale=0.35]{sp_classification-fig5a.png}}
  {\includegraphics[scale=0.35]{sp_classification-fig5b.png}}
  \label{fig:spectra_classification-fig5}
\end{figure}
